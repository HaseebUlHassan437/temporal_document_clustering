{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81848e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Extracted successfully!\n",
      "Files inside extracted folder: ['emails.csv']\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Path to your dataset zip\n",
    "zip_path = \"../data/archive.zip\"\n",
    "extract_dir = \"../data/enron_extracted\"\n",
    "\n",
    "# Step 1: Extract if not already extracted\n",
    "if not os.path.exists(extract_dir):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_dir)\n",
    "    print(\"✅ Extracted successfully!\")\n",
    "else:\n",
    "    print(\"✅ Already extracted!\")\n",
    "\n",
    "# Step 2: Check what files are inside\n",
    "files = os.listdir(extract_dir)\n",
    "print(\"Files inside extracted folder:\", files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fa3cb35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of sample: (5000, 2)\n",
      "\n",
      "Column names:\n",
      " Index(['file', 'message'], dtype='object')\n",
      "\n",
      "First few rows:\n",
      "                       file                                            message\n",
      "0     allen-p/_sent_mail/1.  Message-ID: <18782981.1075855378110.JavaMail.e...\n",
      "1    allen-p/_sent_mail/10.  Message-ID: <15464986.1075855378456.JavaMail.e...\n",
      "2   allen-p/_sent_mail/100.  Message-ID: <24216240.1075855687451.JavaMail.e...\n",
      "3  allen-p/_sent_mail/1000.  Message-ID: <13505866.1075863688222.JavaMail.e...\n",
      "4  allen-p/_sent_mail/1001.  Message-ID: <30922949.1075863688243.JavaMail.e...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = \"../data/enron_extracted/emails.csv\"\n",
    "\n",
    "# Load only first 5000 rows to test (since file is huge)\n",
    "df_sample = pd.read_csv(file_path, nrows=5000)\n",
    "\n",
    "print(\"Shape of sample:\", df_sample.shape)\n",
    "print(\"\\nColumn names:\\n\", df_sample.columns)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df_sample.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08a5740c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of sample: (5000, 2)\n",
      "\n",
      "Column names:\n",
      " Index(['file', 'message'], dtype='object')\n",
      "\n",
      "First few rows:\n",
      "                       file                                            message\n",
      "0     allen-p/_sent_mail/1.  Message-ID: <18782981.1075855378110.JavaMail.e...\n",
      "1    allen-p/_sent_mail/10.  Message-ID: <15464986.1075855378456.JavaMail.e...\n",
      "2   allen-p/_sent_mail/100.  Message-ID: <24216240.1075855687451.JavaMail.e...\n",
      "3  allen-p/_sent_mail/1000.  Message-ID: <13505866.1075863688222.JavaMail.e...\n",
      "4  allen-p/_sent_mail/1001.  Message-ID: <30922949.1075863688243.JavaMail.e...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = \"../data/enron_extracted/emails.csv\"\n",
    "\n",
    "# Load only first 5000 rows to test\n",
    "df_sample = pd.read_csv(file_path, nrows=5000)\n",
    "\n",
    "print(\"Shape of sample:\", df_sample.shape)\n",
    "print(\"\\nColumn names:\\n\", df_sample.columns)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df_sample.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d15a9e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_email(text):\n",
    "    \"\"\"\n",
    "    Parse raw email text into structured fields.\n",
    "    \"\"\"\n",
    "    headers, _, body = text.partition(\"\\n\\n\")  # split headers from body\n",
    "\n",
    "    parsed = {\n",
    "        \"Message-ID\": None,\n",
    "        \"Date\": None,\n",
    "        \"From\": None,\n",
    "        \"To\": None,\n",
    "        \"Subject\": None,\n",
    "        \"Body\": body.strip()\n",
    "    }\n",
    "\n",
    "    for line in headers.split(\"\\n\"):\n",
    "        if line.startswith(\"Message-ID:\"):\n",
    "            parsed[\"Message-ID\"] = line.replace(\"Message-ID:\", \"\").strip()\n",
    "        elif line.startswith(\"Date:\"):\n",
    "            parsed[\"Date\"] = line.replace(\"Date:\", \"\").strip()\n",
    "        elif line.startswith(\"From:\"):\n",
    "            parsed[\"From\"] = line.replace(\"From:\", \"\").strip()\n",
    "        elif line.startswith(\"To:\"):\n",
    "            parsed[\"To\"] = line.replace(\"To:\", \"\").strip()\n",
    "        elif line.startswith(\"Subject:\"):\n",
    "            parsed[\"Subject\"] = line.replace(\"Subject:\", \"\").strip()\n",
    "\n",
    "    return parsed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40144703",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6381e681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Message-ID': '<18782981.1075855378110.JavaMail.evans@thyme>', 'Date': 'Mon, 14 May 2001 16:39:00 -0700 (PDT)', 'From': 'phillip.allen@enron.com', 'To': 'tim.belden@enron.com', 'Subject': '', 'Body': 'Here is our forecast'}\n"
     ]
    }
   ],
   "source": [
    "# from src.preprocessing import parse_email\n",
    "\n",
    "sample_text = df_sample.loc[0, \"message\"]\n",
    "parsed = parse_email(sample_text)\n",
    "print(parsed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "56957f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed DataFrame shape: (5000, 6)\n",
      "\n",
      "Columns: Index(['Message-ID', 'Date', 'From', 'To', 'Subject', 'Body'], dtype='object')\n",
      "\n",
      "Sample parsed rows:\n",
      "                                      Message-ID  \\\n",
      "0  <18782981.1075855378110.JavaMail.evans@thyme>   \n",
      "1  <15464986.1075855378456.JavaMail.evans@thyme>   \n",
      "2  <24216240.1075855687451.JavaMail.evans@thyme>   \n",
      "3  <13505866.1075863688222.JavaMail.evans@thyme>   \n",
      "4  <30922949.1075863688243.JavaMail.evans@thyme>   \n",
      "\n",
      "                                    Date                     From  \\\n",
      "0  Mon, 14 May 2001 16:39:00 -0700 (PDT)  phillip.allen@enron.com   \n",
      "1   Fri, 4 May 2001 13:51:00 -0700 (PDT)  phillip.allen@enron.com   \n",
      "2  Wed, 18 Oct 2000 03:00:00 -0700 (PDT)  phillip.allen@enron.com   \n",
      "3  Mon, 23 Oct 2000 06:13:00 -0700 (PDT)  phillip.allen@enron.com   \n",
      "4  Thu, 31 Aug 2000 05:07:00 -0700 (PDT)  phillip.allen@enron.com   \n",
      "\n",
      "                        To    Subject  \\\n",
      "0     tim.belden@enron.com              \n",
      "1  john.lavorato@enron.com        Re:   \n",
      "2   leah.arsdall@enron.com   Re: test   \n",
      "3    randall.gay@enron.com              \n",
      "4     greg.piper@enron.com  Re: Hello   \n",
      "\n",
      "                                                Body  \n",
      "0                               Here is our forecast  \n",
      "1  Traveling to have a business meeting takes the...  \n",
      "2                     test successful.  way to go!!!  \n",
      "3  Randy,\\n\\n Can you send me a schedule of the s...  \n",
      "4                  Let's shoot for Tuesday at 11:45.  \n"
     ]
    }
   ],
   "source": [
    "parsed_rows = df_sample[\"message\"].apply(parse_email)\n",
    "df_parsed = pd.DataFrame(parsed_rows.tolist())\n",
    "\n",
    "print(\"Parsed DataFrame shape:\", df_parsed.shape)\n",
    "print(\"\\nColumns:\", df_parsed.columns)\n",
    "print(\"\\nSample parsed rows:\")\n",
    "print(df_parsed.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "db5ae4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Body  \\\n",
      "0                               Here is our forecast   \n",
      "1  Traveling to have a business meeting takes the...   \n",
      "2                     test successful.  way to go!!!   \n",
      "3  Randy,\\n\\n Can you send me a schedule of the s...   \n",
      "4                  Let's shoot for Tuesday at 11:45.   \n",
      "\n",
      "                                          clean_body  \n",
      "0                                           forecast  \n",
      "1  travel business meeting take fun trip especial...  \n",
      "2                                test successful way  \n",
      "3  randy \\n\\n  send schedule salary level schedul...  \n",
      "4                           let shoot tuesday         \n"
     ]
    }
   ],
   "source": [
    "# from src.preprocessing import clean_text\n",
    "\n",
    "df_parsed[\"clean_body\"] = df_parsed[\"Body\"].apply(clean_text)\n",
    "print(df_parsed[[\"Body\", \"clean_body\"]].head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5e451db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "\n",
    "# Load spaCy small English model (download first time: python -m spacy download en_core_web_sm)\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"])  # only tokenizer + tagger + lemmatizer\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean email body text for NLP tasks.\n",
    "    Steps: lowercase, remove non-alphabetic, remove stopwords, lemmatize\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)  # keep only letters\n",
    "\n",
    "    doc = nlp(text)\n",
    "    tokens = [\n",
    "        token.lemma_ for token in doc\n",
    "        if not token.is_stop and len(token) > 2\n",
    "    ]\n",
    "\n",
    "    return \" \".join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "60036711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF shape: (5000, 5000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # limit to top 5000 terms for speed\n",
    "\n",
    "# Fit and transform on clean_body\n",
    "X_tfidf = vectorizer.fit_transform(df_parsed[\"clean_body\"])\n",
    "\n",
    "print(\"TF-IDF shape:\", X_tfidf.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9e46bd19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          clean_body  cluster\n",
      "0                                           forecast        5\n",
      "1  travel business meeting take fun trip especial...        5\n",
      "2                                test successful way        5\n",
      "3  randy \\n\\n  send schedule salary level schedul...        5\n",
      "4                           let shoot tuesday               5\n",
      "5                greg \\n\\n  tuesday thursday phillip        5\n",
      "6  follow distribution list update phillip allen ...        3\n",
      "7                                  morning                  5\n",
      "8      login pallen davis \\n\\n  don think require...        5\n",
      "9                          forward phillip allen ...        4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Define number of clusters (you can tune this, e.g., 5, 10, 20)\n",
    "num_clusters = 10  \n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\n",
    "\n",
    "# Fit the model\n",
    "kmeans.fit(X_tfidf)\n",
    "\n",
    "# Assign cluster labels to each email\n",
    "df_parsed[\"cluster\"] = kmeans.labels_\n",
    "\n",
    "print(df_parsed[[\"clean_body\", \"cluster\"]].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "63de0b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: request, resource, approval, com, itcapps, enron, srrs, auth, emaillink, act\n",
      "Cluster 1: ect, john, arnold, enron, hou, subject, com, fraser, jennifer, margaret\n",
      "Cluster 2: ect, hou, enron, allen, phillip, forward, corp, subject, ee, pdx\n",
      "Cluster 3: com, phillip, allen, enron, austin, pallen, forward, ect, subject, loan\n",
      "Cluster 4: enron, need, gas, price, com, know, phillip, let, number, year\n",
      "Cluster 5: phillip, thank, john, work, west, new, send, email, today, desk\n",
      "Cluster 6: image, click, iwon, receive, email, mail, unsubscribe, com, amazon, online\n",
      "Cluster 7: message, gas, file, daily, com, phillip, accenture, recipient, doc, mail\n",
      "Cluster 8: lucy, rent, rentroll, pay, file, deposit, phillip, miss, week, tenant\n",
      "Cluster 9: com, http, www, carrfut, zdnet, zdnetonebox, free, pdf, research, soblander\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Get feature names (words)\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "# For each cluster, get top n terms\n",
    "def get_top_terms_per_cluster(kmeans, terms, n=10):\n",
    "    top_terms = {}\n",
    "    for i, center in enumerate(kmeans.cluster_centers_):\n",
    "        top_idx = center.argsort()[::-1][:n]\n",
    "        top_terms[i] = [terms[j] for j in top_idx]\n",
    "    return top_terms\n",
    "\n",
    "top_terms = get_top_terms_per_cluster(kmeans, terms, n=10)\n",
    "\n",
    "# Print top words per cluster\n",
    "for cluster, words in top_terms.items():\n",
    "    print(f\"Cluster {cluster}: {', '.join(words)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196e1ff7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
